{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shristy183/transfer-learning-project/blob/main/transfer_learning_pipeline_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4438285",
      "metadata": {
        "id": "d4438285"
      },
      "source": [
        "# RAT Rejected Article Tracker – Crossref & OpenAlex Matcher (Colab)\n",
        "\n",
        "Upload your **rejected article tracker (RAT) CSV**; the pipeline finds matches in **both Crossref and OpenAlex**, compares which source has more data, and outputs a CSV like `doi_results_fuzzy` with **Original_Title**, **Crossref_DOI** / **OpenAlex_DOI** (or \"Not Found\"), **Crossref_Found** / **OpenAlex_Found**, **best_doi**, **best_journal**, **publish_year**, **citations**, **match_score**, **source**, **best_link**, **matched_title**. Rejected-only rows (non-empty REJECT_REASON); disk cache for both APIs to save credits.\n",
        "\n",
        "- **Rejected manuscripts**: Uses your CSV columns `MANUSCRIPT_ID`, `REJECT_REASON`, `TITLE`, `FIRST_AUTHOR`; only rows with non-empty `REJECT_REASON` are processed.\n",
        "- **Fewer API calls**: Disk cache + deduplication by (title, first author) and by DOI so re-runs and duplicate rows don’t exhaust OpenAlex/CrossRef free credits.\n",
        "- **Faster**: One OpenAlex search per unique (title, first author); one metadata request per unique DOI.\n",
        "\n",
        "1. Install deps → 2. Config → 3. Upload CSV → 4. Run pipeline → 5. Download output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj25grwvOcjc"
      },
      "source": [
        "## 1. Install dependencies"
      ],
      "id": "Oj25grwvOcjc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBfWQVhaOcjc"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers pandas numpy requests tqdm"
      ],
      "id": "tBfWQVhaOcjc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvQwgBHxOcjd"
      },
      "source": [
        "## 2. Configuration"
      ],
      "id": "hvQwgBHxOcjd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4ed367",
      "metadata": {
        "id": "9d4ed367"
      },
      "outputs": [],
      "source": [
        "# ---------- CONFIG (edit as needed) ----------\n",
        "OPENALEX_WORKS_SEARCH = \"https://api.openalex.org/works\"\n",
        "OPENALEX_WORKS_BY_DOI = \"https://api.openalex.org/works/https://doi.org/{doi}\"\n",
        "CROSSREF_WORKS = \"https://api.crossref.org/works\"\n",
        "PER_PAGE = 5\n",
        "CROSSREF_ROWS = 5\n",
        "MAILTO = \"your_email@example.com\"  # Set for OpenAlex polite pool\n",
        "USER_AGENT = \"TransferLearningRAT/1.0 (mailto:Shristyranjan108@example.com)\"  # Crossref polite pool\n",
        "REQUEST_TIMEOUT_SEC = 30\n",
        "API_DELAY_SEC = 0.3   # Delay when not using parallel (reduced for speed)\n",
        "MAX_WORKERS = 5  # Parallel API requests per source (OpenAlex / Crossref polite pool)\n",
        "TOP_K_CANDIDATES = 5\n",
        "MAX_RETRIES = 3\n",
        "RETRY_BACKOFF_SEC = 2.0\n",
        "\n",
        "# ---------- CACHE (saves API credits; reuse across runs) ----------\n",
        "USE_DISK_CACHE = True\n",
        "CACHE_DIR = \"/content/transfer_learning_cache\"\n",
        "SEARCH_CACHE_FILE = \"openalex_search_cache.json\"\n",
        "DOI_CACHE_FILE = \"openalex_doi_cache.json\"\n",
        "CROSSREF_SEARCH_CACHE_FILE = \"crossref_search_cache.json\"\n",
        "CROSSREF_DOI_CACHE_FILE = \"crossref_doi_cache.json\"\n",
        "\n",
        "# ---------- REJECTED CSV COLUMNS (must match your file) ----------\n",
        "COL_MANUSCRIPT_ID = \"MANUSCRIPT_ID\"\n",
        "COL_DATE_REJECTION = \"DATE_OF_REJECTION\"\n",
        "COL_REJECT_REASON = \"REJECT_REASON\"\n",
        "COL_TITLE = \"TITLE\"\n",
        "COL_FIRST_AUTHOR = \"FIRST_AUTHOR\"\n",
        "COL_CORRESPONDING_AUTHOR = \"CORRESPONDING_AUTHOR\"\n",
        "COL_CO_AUTHORS = \"CO_AUTHORS\"\n",
        "\n",
        "MODEL_NAME = \"intfloat/e5-large-v2\"\n",
        "QUERY_PREFIX = \"query: \"\n",
        "PASSAGE_PREFIX = \"passage: \"\n",
        "BATCH_SIZE = 64\n",
        "MAX_LENGTH = 512\n",
        "NORMALIZE_EMBEDDINGS = True\n",
        "\n",
        "SIMILARITY_THRESHOLD = 0.80\n",
        "OUTPUT_FILENAME = \"doi_results_fuzzy.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f25ff76",
      "metadata": {
        "id": "5f25ff76"
      },
      "source": [
        "## 3. Retrieval (OpenAlex + Crossref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef343742",
      "metadata": {
        "id": "ef343742"
      },
      "outputs": [],
      "source": [
        "# ---------- Disk cache to avoid exhausting OpenAlex/CrossRef free credits ----------\n",
        "import json\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "def _cache_path(filename):\n",
        "    d = Path(CACHE_DIR)\n",
        "    if USE_DISK_CACHE:\n",
        "        d.mkdir(parents=True, exist_ok=True)\n",
        "    return d / filename\n",
        "\n",
        "def _norm_key(s):\n",
        "    return \" \".join(str(s).strip().lower().split()) if s else \"\"\n",
        "\n",
        "def _search_cache_key(title, first_author=\"\"):\n",
        "    raw = _norm_key(title) + \"|\" + _norm_key(first_author)\n",
        "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def load_search_cache():\n",
        "    p = _cache_path(SEARCH_CACHE_FILE)\n",
        "    if not p.exists():\n",
        "        return {}\n",
        "    try:\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "def save_search_cache(cache):\n",
        "    p = _cache_path(SEARCH_CACHE_FILE)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cache, f, ensure_ascii=False, indent=0)\n",
        "\n",
        "def load_doi_cache():\n",
        "    p = _cache_path(DOI_CACHE_FILE)\n",
        "    if not p.exists():\n",
        "        return {}\n",
        "    try:\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            return json.load(f)\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "def save_doi_cache(cache):\n",
        "    p = _cache_path(DOI_CACHE_FILE)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cache, f, ensure_ascii=False, indent=0)\n",
        "\n",
        "def load_crossref_search_cache():\n",
        "    p = _cache_path(CROSSREF_SEARCH_CACHE_FILE)\n",
        "    if not p.exists(): return {}\n",
        "    try:\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
        "    except Exception: return {}\n",
        "\n",
        "def save_crossref_search_cache(cache):\n",
        "    p = _cache_path(CROSSREF_SEARCH_CACHE_FILE)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cache, f, ensure_ascii=False, indent=0)\n",
        "\n",
        "def load_crossref_doi_cache():\n",
        "    p = _cache_path(CROSSREF_DOI_CACHE_FILE)\n",
        "    if not p.exists(): return {}\n",
        "    try:\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
        "    except Exception: return {}\n",
        "\n",
        "def save_crossref_doi_cache(cache):\n",
        "    p = _cache_path(CROSSREF_DOI_CACHE_FILE)\n",
        "    p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(p, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cache, f, ensure_ascii=False, indent=0)\n",
        "\n",
        "print(\"Cache dir:\", CACHE_DIR, \"(enabled)\" if USE_DISK_CACHE else \"(disabled)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274c1a37",
      "metadata": {
        "id": "274c1a37"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def _extract_doi(work):\n",
        "    doi_url = work.get(\"doi\")\n",
        "    if not doi_url or not isinstance(doi_url, str):\n",
        "        return None\n",
        "    if doi_url.startswith(\"https://doi.org/\"):\n",
        "        return doi_url.replace(\"https://doi.org/\", \"\", 1).strip()\n",
        "    return doi_url.strip() or None\n",
        "\n",
        "def search_openalex(title):\n",
        "    params = {\"search\": title, \"per-page\": PER_PAGE, \"mailto\": MAILTO}\n",
        "    candidates = []\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = requests.get(OPENALEX_WORKS_SEARCH, params=params, timeout=REQUEST_TIMEOUT_SEC)\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json()\n",
        "            break\n",
        "        except requests.RequestException:\n",
        "            if attempt == MAX_RETRIES - 1:\n",
        "                return []\n",
        "            time.sleep(RETRY_BACKOFF_SEC * (attempt + 1))\n",
        "    else:\n",
        "        return []\n",
        "    for hit in (data.get(\"results\") or []):\n",
        "        doi = _extract_doi(hit)\n",
        "        if doi is None:\n",
        "            continue\n",
        "        display_title = (hit.get(\"title\") or \"\").strip() or \"(no title)\"\n",
        "        candidates.append({\"title\": display_title, \"doi\": doi, \"source\": \"OpenAlex\"})\n",
        "        if len(candidates) >= TOP_K_CANDIDATES:\n",
        "            break\n",
        "    return candidates\n",
        "\n",
        "def retrieve_candidates_for_titles(titles, first_authors=None, delay_sec=API_DELAY_SEC):\n",
        "    \"\"\"Retrieve OpenAlex candidates with disk cache, deduplication, and parallel API calls.\"\"\"\n",
        "    n = len(titles)\n",
        "    if first_authors is None: first_authors = [\"\"] * n\n",
        "    first_authors = list(first_authors) if len(first_authors) == n else [\"\"] * n\n",
        "    key_to_indices = {}\n",
        "    for i in range(n):\n",
        "        t = str(titles[i]).strip() if titles[i] else \"\"\n",
        "        fa = str(first_authors[i]).strip() if first_authors[i] else \"\"\n",
        "        key = _search_cache_key(t, fa)\n",
        "        if key not in key_to_indices: key_to_indices[key] = []\n",
        "        key_to_indices[key].append((i, t))\n",
        "    cache = load_search_cache() if USE_DISK_CACHE else {}\n",
        "    cache_updated = False\n",
        "    key_to_candidates = {}\n",
        "    for key in key_to_indices:\n",
        "        if key in cache:\n",
        "            key_to_candidates[key] = cache[key]\n",
        "            continue\n",
        "        query_title = (key_to_indices[key][0][1] or \"\").strip()\n",
        "        if not query_title:\n",
        "            key_to_candidates[key] = []\n",
        "            continue\n",
        "    keys_to_fetch = [(k, key_to_indices[k][0][1]) for k in key_to_indices if k not in key_to_candidates and (key_to_indices[k][0][1] or \"\").strip()]\n",
        "    if keys_to_fetch:\n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "            future_to_key = {ex.submit(search_openalex, qt): k for k, qt in keys_to_fetch}\n",
        "            for future in tqdm(as_completed(future_to_key), total=len(future_to_key), desc=\"OpenAlex (parallel)\"):\n",
        "                key = future_to_key[future]\n",
        "                try:\n",
        "                    cands = future.result()\n",
        "                    key_to_candidates[key] = cands\n",
        "                    if USE_DISK_CACHE: cache[key] = cands; cache_updated = True\n",
        "                except Exception: key_to_candidates[key] = []\n",
        "    if USE_DISK_CACHE and cache_updated: save_search_cache(cache)\n",
        "    results = [key_to_candidates.get(_search_cache_key(str(titles[i]).strip() if titles[i] else \"\", str(first_authors[i]).strip() if first_authors[i] else \"\"), []) for i in range(n)]\n",
        "    return results\n",
        "\n",
        "# ---------- Crossref search (cached + deduplicated) ----------\n",
        "def search_crossref(title):\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    params = {\"query.bibliographic\": title, \"rows\": CROSSREF_ROWS}\n",
        "    candidates = []\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = requests.get(CROSSREF_WORKS, params=params, headers=headers, timeout=REQUEST_TIMEOUT_SEC)\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json()\n",
        "            break\n",
        "        except requests.RequestException:\n",
        "            if attempt == MAX_RETRIES - 1: return []\n",
        "            time.sleep(RETRY_BACKOFF_SEC * (attempt + 1))\n",
        "    else:\n",
        "        return []\n",
        "    for item in (data.get(\"message\", {}).get(\"items\") or [])[:TOP_K_CANDIDATES]:\n",
        "        doi = (item.get(\"DOI\") or \"\").strip()\n",
        "        if not doi: continue\n",
        "        titles_list = item.get(\"title\") or []\n",
        "        display_title = (titles_list[0] if titles_list else \"\").strip() or \"(no title)\"\n",
        "        candidates.append({\"title\": display_title, \"doi\": doi, \"source\": \"Crossref\"})\n",
        "    return candidates\n",
        "\n",
        "def retrieve_crossref_candidates_for_titles(titles, first_authors=None, delay_sec=API_DELAY_SEC):\n",
        "    \"\"\"Retrieve Crossref candidates with disk cache, deduplication, and parallel API calls.\"\"\"\n",
        "    n = len(titles)\n",
        "    if first_authors is None: first_authors = [\"\"] * n\n",
        "    first_authors = list(first_authors) if len(first_authors) == n else [\"\"] * n\n",
        "    key_to_indices = {}\n",
        "    for i in range(n):\n",
        "        t = str(titles[i]).strip() if titles[i] else \"\"\n",
        "        fa = str(first_authors[i]).strip() if first_authors[i] else \"\"\n",
        "        key = _search_cache_key(t, fa)\n",
        "        if key not in key_to_indices: key_to_indices[key] = []\n",
        "        key_to_indices[key].append((i, t))\n",
        "    cache = load_crossref_search_cache() if USE_DISK_CACHE else {}\n",
        "    cache_updated = False\n",
        "    key_to_candidates = {}\n",
        "    for key in key_to_indices:\n",
        "        if key in cache:\n",
        "            key_to_candidates[key] = cache[key]\n",
        "            continue\n",
        "        query_title = (key_to_indices[key][0][1] or \"\").strip()\n",
        "        if not query_title:\n",
        "            key_to_candidates[key] = []\n",
        "            continue\n",
        "    keys_to_fetch = [(k, key_to_indices[k][0][1]) for k in key_to_indices if k not in key_to_candidates and (key_to_indices[k][0][1] or \"\").strip()]\n",
        "    if keys_to_fetch:\n",
        "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
        "            future_to_key = {ex.submit(search_crossref, qt): k for k, qt in keys_to_fetch}\n",
        "            for future in tqdm(as_completed(future_to_key), total=len(future_to_key), desc=\"Crossref (parallel)\"):\n",
        "                key = future_to_key[future]\n",
        "                try:\n",
        "                    cands = future.result()\n",
        "                    key_to_candidates[key] = cands\n",
        "                    if USE_DISK_CACHE: cache[key] = cands; cache_updated = True\n",
        "                except Exception: key_to_candidates[key] = []\n",
        "    if USE_DISK_CACHE and cache_updated: save_crossref_search_cache(cache)\n",
        "    results = [key_to_candidates.get(_search_cache_key(str(titles[i]).strip() if titles[i] else \"\", str(first_authors[i]).strip() if first_authors[i] else \"\"), []) for i in range(n)]\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sBJ52lnOcje"
      },
      "source": [
        "## 4. E5 Embeddings (batch)"
      ],
      "id": "0sBJ52lnOcje"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d67e254",
      "metadata": {
        "id": "7d67e254"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Force T4 GPU when available (Runtime → Change runtime type → T4 GPU)\n",
        "def _get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        gpu_name = torch.cuda.get_device_name(0) if torch.cuda.device_count() else \"GPU\"\n",
        "        print(f\"Using device: {device} ({gpu_name})\")\n",
        "        return device\n",
        "    print(\"Using device: cpu (no GPU found)\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "def load_model_and_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "    device = _get_device()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    return model, tokenizer, device\n",
        "\n",
        "def _encode_batch(model, tokenizer, device, texts, prefix, batch_size=BATCH_SIZE, max_length=MAX_LENGTH, normalize=NORMALIZE_EMBEDDINGS):\n",
        "    prefixed = [prefix + (t or \"\") for t in texts]\n",
        "    all_emb = []\n",
        "    for start in range(0, len(prefixed), batch_size):\n",
        "        batch = prefixed[start : start + batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            out = model(**inputs)\n",
        "            mask = inputs[\"attention_mask\"]\n",
        "            last_hidden = out.last_hidden_state\n",
        "            summed = (last_hidden * mask.unsqueeze(-1)).sum(dim=1)\n",
        "            lengths = mask.sum(dim=1, keepdim=True).clamp(min=1e-9)\n",
        "            emb = (summed / lengths).cpu().numpy()\n",
        "        if normalize:\n",
        "            norms = np.linalg.norm(emb, axis=1, keepdims=True)\n",
        "            norms = np.where(norms == 0, 1.0, norms)\n",
        "            emb = emb / norms\n",
        "        all_emb.append(emb)\n",
        "    return np.vstack(all_emb).astype(np.float32)\n",
        "\n",
        "def encode_queries(model, tokenizer, device, titles, batch_size=BATCH_SIZE):\n",
        "    return _encode_batch(model, tokenizer, device, titles, QUERY_PREFIX, batch_size=batch_size)\n",
        "\n",
        "def encode_passages(model, tokenizer, device, titles, batch_size=BATCH_SIZE):\n",
        "    return _encode_batch(model, tokenizer, device, titles, PASSAGE_PREFIX, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x3gLhdAOcjf"
      },
      "source": [
        "## 5. Similarity & ranking"
      ],
      "id": "7x3gLhdAOcjf"
    },
    {
      "cell_type": "markdown",
      "id": "674e9893",
      "metadata": {
        "id": "674e9893"
      },
      "source": [
        "## Load E5 on T4 GPU (run this before Step 1 so Colab shows GPU usage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb18ba5c",
      "metadata": {
        "id": "cb18ba5c"
      },
      "outputs": [],
      "source": [
        "# Load E5 onto GPU *now* so T4 is used and Colab Resources show GPU memory (before long retrieval)\n",
        "print(\"Loading E5 model onto GPU (T4)...\")\n",
        "model, tokenizer, device = load_model_and_tokenizer()\n",
        "# Warmup: one batch so GPU allocates memory\n",
        "encode_queries(model, tokenizer, device, [\"warmup\"])\n",
        "print(\"GPU ready. Run Step 1 (retrieval) next.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca66814",
      "metadata": {
        "id": "dca66814"
      },
      "outputs": [],
      "source": [
        "def build_row_splits(candidates_per_row):\n",
        "    splits = [0]\n",
        "    for cands in candidates_per_row:\n",
        "        splits.append(splits[-1] + len(cands))\n",
        "    return splits\n",
        "\n",
        "def flatten_candidates(candidates_per_row):\n",
        "    titles, dicts = [], []\n",
        "    for cands in candidates_per_row:\n",
        "        for c in cands:\n",
        "            titles.append(c.get(\"title\") or \"\")\n",
        "            dicts.append(c)\n",
        "    return titles, dicts\n",
        "\n",
        "def rank_and_select_best(query_embeddings, passage_embeddings, row_splits, threshold=SIMILARITY_THRESHOLD):\n",
        "    n_queries = query_embeddings.shape[0]\n",
        "    results = []\n",
        "    for i in range(n_queries):\n",
        "        start, end = row_splits[i], row_splits[i + 1]\n",
        "        if start >= end:\n",
        "            results.append((-1, -1.0))\n",
        "            continue\n",
        "        q = query_embeddings[i : i + 1]\n",
        "        p = passage_embeddings[start:end]\n",
        "        scores = np.dot(p, q.T).ravel()\n",
        "        best_local = int(np.argmax(scores))\n",
        "        best_score = float(scores[best_local])\n",
        "        if best_score < threshold:\n",
        "            results.append((-1, best_score))\n",
        "        else:\n",
        "            results.append((start + best_local, best_score))\n",
        "    return results\n",
        "\n",
        "def rank_and_select_best_by_source(query_embeddings, passage_embeddings, row_splits, flat_candidate_dicts, threshold=SIMILARITY_THRESHOLD):\n",
        "    \"\"\"Returns per row: (best_global_idx, best_score, best_oa_global_idx, best_oa_score, best_cr_global_idx, best_cr_score).\"\"\"\n",
        "    n_queries = query_embeddings.shape[0]\n",
        "    results = []\n",
        "    for i in range(n_queries):\n",
        "        start, end = row_splits[i], row_splits[i + 1]\n",
        "        best_oa, best_cr = -1, -1\n",
        "        best_oa_score, best_cr_score = -1.0, -1.0\n",
        "        if start >= end:\n",
        "            results.append((-1, -1.0, -1, -1.0, -1, -1.0))\n",
        "            continue\n",
        "        q = query_embeddings[i : i + 1]\n",
        "        p = passage_embeddings[start:end]\n",
        "        scores = np.dot(p, q.T).ravel()\n",
        "        for j in range(len(scores)):\n",
        "            idx = start + j\n",
        "            s = float(scores[j])\n",
        "            src = (flat_candidate_dicts[idx].get(\"source\") or \"\").strip()\n",
        "            if src == \"OpenAlex\" and s > best_oa_score:\n",
        "                best_oa, best_oa_score = idx, s\n",
        "            if src == \"Crossref\" and s > best_cr_score:\n",
        "                best_cr, best_cr_score = idx, s\n",
        "        best_local = int(np.argmax(scores))\n",
        "        best_global = start + best_local\n",
        "        best_score = float(scores[best_local])\n",
        "        if best_score < threshold:\n",
        "            best_global = -1\n",
        "            best_score = -1.0\n",
        "        results.append((best_global, best_score, best_oa, best_oa_score, best_cr, best_cr_score))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b84c0394",
      "metadata": {
        "id": "b84c0394"
      },
      "source": [
        "## 6. Metadata enrichment (OpenAlex + Crossref by DOI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f220f26",
      "metadata": {
        "id": "4f220f26"
      },
      "outputs": [],
      "source": [
        "def _safe_int(val, default=None):\n",
        "    if val is None: return default\n",
        "    try: return int(val)\n",
        "    except (TypeError, ValueError): return default\n",
        "\n",
        "def _safe_str(val):\n",
        "    return \"\" if val is None else str(val).strip()\n",
        "\n",
        "def _empty_metadata():\n",
        "    return {\"journal\": None, \"publication_year\": None, \"cited_by_count\": None, \"type\": None, \"open_access\": None}\n",
        "\n",
        "def _parse_work(data):\n",
        "    pub_year = _safe_int(data.get(\"publication_year\"))\n",
        "    cited = _safe_int(data.get(\"cited_by_count\"), 0)\n",
        "    type_ = _safe_str(data.get(\"type\"))\n",
        "    journal = None\n",
        "    if \"primary_location\" in data and isinstance(data[\"primary_location\"], dict):\n",
        "        src = data[\"primary_location\"].get(\"source\")\n",
        "        if isinstance(src, dict): journal = _safe_str(src.get(\"display_name\")) or None\n",
        "    if not journal and isinstance(data.get(\"host_venue\"), dict): journal = _safe_str(data[\"host_venue\"].get(\"display_name\")) or None\n",
        "    oa = data.get(\"open_access\")\n",
        "    if oa is None: oa_str = None\n",
        "    elif isinstance(oa, dict):\n",
        "        is_oa = oa.get(\"is_oa\")\n",
        "        oa_str = (\"true\" if is_oa else \"false\") if is_oa is not None else (_safe_str(oa.get(\"status\")) or None)\n",
        "    else: oa_str = _safe_str(oa) if oa else None\n",
        "    return {\"journal\": journal, \"publication_year\": pub_year, \"cited_by_count\": cited, \"type\": type_ or None, \"open_access\": oa_str}\n",
        "\n",
        "def fetch_work_by_doi(doi):\n",
        "    if not doi or not str(doi).strip():\n",
        "        return _empty_metadata()\n",
        "    url = OPENALEX_WORKS_BY_DOI.format(doi=doi.strip())\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = requests.get(url, timeout=REQUEST_TIMEOUT_SEC)\n",
        "            resp.raise_for_status()\n",
        "            return _parse_work(resp.json())\n",
        "        except requests.RequestException:\n",
        "            if attempt == MAX_RETRIES - 1:\n",
        "                return _empty_metadata()\n",
        "            time.sleep(RETRY_BACKOFF_SEC * (attempt + 1))\n",
        "    return _empty_metadata()\n",
        "\n",
        "def get_metadata_by_doi_cached(doi):\n",
        "    \"\"\"Fetch OpenAlex work by DOI with disk cache to save API credits.\"\"\"\n",
        "    doi_key = str(doi).strip() if doi else \"\"\n",
        "    if not doi_key:\n",
        "        return _empty_metadata()\n",
        "    cache = load_doi_cache() if USE_DISK_CACHE else {}\n",
        "    if doi_key in cache:\n",
        "        return cache[doi_key]\n",
        "    meta = fetch_work_by_doi(doi_key)\n",
        "    if USE_DISK_CACHE:\n",
        "        cache[doi_key] = meta\n",
        "        save_doi_cache(cache)\n",
        "    return meta\n",
        "\n",
        "def fetch_metadata_batch_cached(dois, delay_sec=API_DELAY_SEC):\n",
        "    \"\"\"Fetch OpenAlex metadata for DOIs; cache + deduplication.\"\"\"\n",
        "    dois = [str(d).strip() for d in dois if d and str(d).strip()]\n",
        "    unique = list(dict.fromkeys(dois))\n",
        "    cache = load_doi_cache() if USE_DISK_CACHE else {}\n",
        "    cache_updated = False\n",
        "    result = {}\n",
        "    for doi in tqdm(unique, desc=\"OpenAlex DOI metadata (cached)\"):\n",
        "        if doi in cache:\n",
        "            result[doi] = cache[doi]\n",
        "            continue\n",
        "        meta = fetch_work_by_doi(doi)\n",
        "        result[doi] = meta\n",
        "        if USE_DISK_CACHE: cache[doi] = meta; cache_updated = True\n",
        "        if delay_sec > 0: time.sleep(delay_sec)\n",
        "    if USE_DISK_CACHE and cache_updated: save_doi_cache(cache)\n",
        "    return result\n",
        "\n",
        "def _empty_crossref_meta():\n",
        "    return {\"journal\": None, \"publication_year\": None, \"cited_by_count\": None}\n",
        "\n",
        "def _parse_crossref_work(msg):\n",
        "    ct = msg.get(\"container-title\") or []\n",
        "    journal = (ct[0] if ct else \"\").strip() or None\n",
        "    issued = msg.get(\"issued\") or msg.get(\"published-print\") or {}\n",
        "    parts = (issued.get(\"date-parts\") or [[]])[0]\n",
        "    pub_year = _safe_int(parts[0]) if parts else None\n",
        "    cited = _safe_int(msg.get(\"is-referenced-by-count\"), 0)\n",
        "    return {\"journal\": journal, \"publication_year\": pub_year, \"cited_by_count\": cited}\n",
        "\n",
        "def fetch_crossref_by_doi(doi):\n",
        "    from urllib.parse import quote\n",
        "    if not doi or not str(doi).strip(): return _empty_crossref_meta()\n",
        "    url = \"https://api.crossref.org/works/\" + quote(str(doi).strip(), safe=\"\")\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT_SEC)\n",
        "            resp.raise_for_status()\n",
        "            return _parse_crossref_work(resp.json().get(\"message\") or {})\n",
        "        except requests.RequestException:\n",
        "            if attempt == MAX_RETRIES - 1: return _empty_crossref_meta()\n",
        "            time.sleep(RETRY_BACKOFF_SEC * (attempt + 1))\n",
        "    return _empty_crossref_meta()\n",
        "\n",
        "def fetch_crossref_metadata_batch_cached(dois, delay_sec=API_DELAY_SEC):\n",
        "    dois = [str(d).strip() for d in dois if d and str(d).strip()]\n",
        "    unique = list(dict.fromkeys(dois))\n",
        "    cache = load_crossref_doi_cache() if USE_DISK_CACHE else {}\n",
        "    cache_updated = False\n",
        "    result = {}\n",
        "    for doi in tqdm(unique, desc=\"Crossref DOI metadata (cached)\"):\n",
        "        if doi in cache: result[doi] = cache[doi]; continue\n",
        "        meta = fetch_crossref_by_doi(doi)\n",
        "        result[doi] = meta\n",
        "        if USE_DISK_CACHE: cache[doi] = meta; cache_updated = True\n",
        "        if delay_sec > 0: time.sleep(delay_sec)\n",
        "    if USE_DISK_CACHE and cache_updated: save_crossref_doi_cache(cache)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHm_XC59Ocjf"
      },
      "source": [
        "## 7. Upload input CSV & run pipeline"
      ],
      "id": "QHm_XC59Ocjf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWwSpqv1Ocjf"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Option A: Upload CSV from your machine\n",
        "uploaded = files.upload()\n",
        "input_name = list(uploaded.keys())[0]  # use first uploaded file\n",
        "input_path = Path(input_name)\n",
        "\n",
        "# Option B: If you already have file in Colab (e.g. in /content), set it manually:\n",
        "# input_path = Path(\"/content/input_manuscripts.csv\")"
      ],
      "id": "aWwSpqv1Ocjf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df5078f7",
      "metadata": {
        "id": "df5078f7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(input_path)\n",
        "# Required columns (rejected CSV: MANUSCRIPT_ID, REJECT_REASON, TITLE; FIRST_AUTHOR optional for dedupe)\n",
        "for col in (COL_MANUSCRIPT_ID, COL_TITLE):\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"CSV must have column: {col}\")\n",
        "# Identify rejected manuscripts: keep only rows with non-empty REJECT_REASON\n",
        "if COL_REJECT_REASON in df.columns:\n",
        "    df = df[df[COL_REJECT_REASON].notna() & (df[COL_REJECT_REASON].astype(str).str.strip() != \"\")]\n",
        "    print(f\"Filtered to rejected manuscripts only: {len(df)} rows (REJECT_REASON present).\")\n",
        "else:\n",
        "    print(\"Warning: No REJECT_REASON column; processing all rows.\")\n",
        "manuscript_ids = df[COL_MANUSCRIPT_ID].astype(str).tolist()\n",
        "titles = df[COL_TITLE].fillna(\"\").astype(str).tolist()\n",
        "first_authors = df[COL_FIRST_AUTHOR].fillna(\"\").astype(str).tolist() if COL_FIRST_AUTHOR in df.columns else [\"\"] * len(df)\n",
        "n_rows = len(titles)\n",
        "print(f\"Loaded {n_rows} rows. Running pipeline...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41bc8dc",
      "metadata": {
        "id": "e41bc8dc"
      },
      "outputs": [],
      "source": [
        "print(\"Step 1: Retrieving candidates from OpenAlex and Crossref (parallel, cached + deduplicated)...\")\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "with ThreadPoolExecutor(max_workers=2) as ex:\n",
        "    f_oa = ex.submit(retrieve_candidates_for_titles, titles, first_authors)\n",
        "    f_cr = ex.submit(retrieve_crossref_candidates_for_titles, titles, first_authors)\n",
        "    oa_per_row = f_oa.result()\n",
        "    cr_per_row = f_cr.result()\n",
        "candidates_per_row = [oa_per_row[i] + cr_per_row[i] for i in range(n_rows)]\n",
        "row_splits = build_row_splits(candidates_per_row)\n",
        "passage_titles, flat_candidate_dicts = flatten_candidates(candidates_per_row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d70bb6",
      "metadata": {
        "id": "88d70bb6"
      },
      "outputs": [],
      "source": [
        "print(\"Step 2: Computing embeddings on GPU (batch)...\")\n",
        "query_embeddings = encode_queries(model, tokenizer, device, titles)\n",
        "if passage_titles:\n",
        "    passage_embeddings = encode_passages(model, tokenizer, device, passage_titles)\n",
        "else:\n",
        "    passage_embeddings = np.zeros((0, query_embeddings.shape[1]), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c3a307",
      "metadata": {
        "id": "d3c3a307"
      },
      "outputs": [],
      "source": [
        "print(\"Step 3: Ranking by semantic similarity (per source: OpenAlex & Crossref)...\")\n",
        "best_per_row = rank_and_select_best_by_source(query_embeddings, passage_embeddings, row_splits, flat_candidate_dicts)\n",
        "\n",
        "def _get_doi_or_not_found(global_idx, flat_dicts):\n",
        "    if global_idx < 0: return \"Not Found\"\n",
        "    d = flat_dicts[global_idx]\n",
        "    return (d.get(\"doi\") or \"\").strip() or \"Not Found\"\n",
        "\n",
        "Crossref_DOI_per_row = [_get_doi_or_not_found(best_per_row[i][4], flat_candidate_dicts) for i in range(n_rows)]\n",
        "OpenAlex_DOI_per_row = [_get_doi_or_not_found(best_per_row[i][2], flat_candidate_dicts) for i in range(n_rows)]\n",
        "Crossref_Found = [\"Yes\" if best_per_row[i][4] >= 0 else \"No\" for i in range(n_rows)]\n",
        "OpenAlex_Found = [\"Yes\" if best_per_row[i][2] >= 0 else \"No\" for i in range(n_rows)]\n",
        "\n",
        "best_global_per_row = [best_per_row[i][0] for i in range(n_rows)]\n",
        "best_score_per_row = [best_per_row[i][1] for i in range(n_rows)]\n",
        "matched_title_per_row = [(flat_candidate_dicts[best_global_per_row[i]].get(\"title\") or \"\") if best_global_per_row[i] >= 0 else \"\" for i in range(n_rows)]\n",
        "matched_doi_per_row = [(flat_candidate_dicts[best_global_per_row[i]].get(\"doi\") or None) if best_global_per_row[i] >= 0 else None for i in range(n_rows)]\n",
        "source_per_row = [(flat_candidate_dicts[best_global_per_row[i]].get(\"source\") or \"\") if best_global_per_row[i] >= 0 else \"\" for i in range(n_rows)]\n",
        "score_per_row = [round(best_score_per_row[i], 4) if best_score_per_row[i] >= 0 else None for i in range(n_rows)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0990c132",
      "metadata": {
        "id": "0990c132"
      },
      "outputs": [],
      "source": [
        "print(\"Step 4: Fetching metadata for best matches (OpenAlex + Crossref, cached)...\")\n",
        "dois_oa = list(dict.fromkeys([str(matched_doi_per_row[i]).strip() for i in range(n_rows) if source_per_row[i] == \"OpenAlex\" and matched_doi_per_row[i] and str(matched_doi_per_row[i]).strip()]))\n",
        "dois_cr = list(dict.fromkeys([str(matched_doi_per_row[i]).strip() for i in range(n_rows) if source_per_row[i] == \"Crossref\" and matched_doi_per_row[i] and str(matched_doi_per_row[i]).strip()]))\n",
        "oa_meta = fetch_metadata_batch_cached(dois_oa) if dois_oa else {}\n",
        "cr_meta = fetch_crossref_metadata_batch_cached(dois_cr) if dois_cr else {}\n",
        "metadata_per_row = []\n",
        "for i in range(n_rows):\n",
        "    doi = matched_doi_per_row[i]\n",
        "    key = str(doi).strip() if doi else \"\"\n",
        "    if source_per_row[i] == \"Crossref\" and key:\n",
        "        m = cr_meta.get(key, _empty_crossref_meta())\n",
        "        metadata_per_row.append({\"journal\": m.get(\"journal\"), \"publication_year\": m.get(\"publication_year\"), \"cited_by_count\": m.get(\"cited_by_count\")})\n",
        "    elif key:\n",
        "        m = oa_meta.get(key, _empty_metadata())\n",
        "        metadata_per_row.append({\"journal\": m.get(\"journal\"), \"publication_year\": m.get(\"publication_year\"), \"cited_by_count\": m.get(\"cited_by_count\")})\n",
        "    else:\n",
        "        metadata_per_row.append({\"journal\": None, \"publication_year\": None, \"cited_by_count\": None})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3092595f",
      "metadata": {
        "id": "3092595f"
      },
      "outputs": [],
      "source": [
        "best_link_per_row = [(\"https://doi.org/\" + str(matched_doi_per_row[i])) if matched_doi_per_row[i] else \"\" for i in range(n_rows)]\n",
        "best_doi_str = [str(matched_doi_per_row[i]) if matched_doi_per_row[i] else \"\" for i in range(n_rows)]\n",
        "\n",
        "OUTPUT_COLUMNS = [\"MANUSCRIPT_ID\", \"Original_Title\", \"Crossref_DOI\", \"OpenAlex_DOI\", \"Crossref_Found\", \"OpenAlex_Found\", \"best_doi\", \"best_journal\", \"publish_year\", \"citations\", \"match_score\", \"source\", \"best_link\", \"matched_title\"]\n",
        "rows = []\n",
        "for i in range(n_rows):\n",
        "    meta = metadata_per_row[i]\n",
        "    rows.append({\n",
        "        \"MANUSCRIPT_ID\": manuscript_ids[i],\n",
        "        \"Original_Title\": titles[i],\n",
        "        \"Crossref_DOI\": Crossref_DOI_per_row[i],\n",
        "        \"OpenAlex_DOI\": OpenAlex_DOI_per_row[i],\n",
        "        \"Crossref_Found\": Crossref_Found[i],\n",
        "        \"OpenAlex_Found\": OpenAlex_Found[i],\n",
        "        \"best_doi\": best_doi_str[i],\n",
        "        \"best_journal\": meta.get(\"journal\") or \"\",\n",
        "        \"publish_year\": meta.get(\"publication_year\") if meta.get(\"publication_year\") is not None else \"\",\n",
        "        \"citations\": meta.get(\"cited_by_count\") if meta.get(\"cited_by_count\") is not None else \"\",\n",
        "        \"match_score\": score_per_row[i] if score_per_row[i] is not None else \"\",\n",
        "        \"source\": source_per_row[i],\n",
        "        \"best_link\": best_link_per_row[i],\n",
        "        \"matched_title\": matched_title_per_row[i],\n",
        "    })\n",
        "out_df = pd.DataFrame(rows, columns=OUTPUT_COLUMNS)\n",
        "out_df.to_csv(OUTPUT_FILENAME, index=False)\n",
        "print(f\"Step 5: Saved {len(out_df)} rows to {OUTPUT_FILENAME}\")\n",
        "print(\"Summary: Crossref found\", sum(1 for x in Crossref_Found if x == \"Yes\"), \"; OpenAlex found\", sum(1 for x in OpenAlex_Found if x == \"Yes\"))\n",
        "out_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryIYmlsMOcjg"
      },
      "source": [
        "## 8. Download output CSV"
      ],
      "id": "ryIYmlsMOcjg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr3RnJcBOcjg"
      },
      "outputs": [],
      "source": [
        "files.download(OUTPUT_FILENAME)"
      ],
      "id": "fr3RnJcBOcjg"
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
